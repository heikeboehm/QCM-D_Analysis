{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36cb9459-f1c1-499a-b654-9c0f084596e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Program Description:\n",
    "This program breaks down the data recorded on a QSense-Analyzer into independent, unified data sets, for each\n",
    "Sensor position. It processes QCM-D data obtained with Q-Tools Software (Version 2.8.5) on QSense-Analyzer with open modules.\n",
    "\n",
    "Author:\n",
    "- Heike Böhm, GlycoScience group, Department of Cellular Biophysics, MPI for Medical Research (MPImF-CBP-GS)\n",
    "- With strong support from Martin Schröter in the same department.\n",
    "\n",
    "Input Data Sources:\n",
    "- Rawdata as tab spaced CSV file exported from Q-Tools using \",\" as decimal points\n",
    "    expects data to be saved with Data_ID_rawdata.txt\n",
    "- Corresponding timeline copied and pasted from the notes window of Q-Tools into a plain text file.\n",
    "    expects data to be saved with Data_ID_timeline.txt\n",
    "    Format: h:mm:ss S# description. Timepoints should correspond to solution changes within a 4-minute timeframe.\n",
    "\n",
    "Output Generated:\n",
    "- A CSV file for each sensor file containing the following data: \n",
    "    - adj_qcmd_data: \n",
    "        Adjusted QCM-D dataset with time in seconds, normalized Δf/n, and normalized D for all overtones. \n",
    "          Normalization is achieved by subtracting the averaged value of the first 100 data points \n",
    "          for each measured frequency and dissipation overtone.\n",
    "          All normalized frequency values have been divided by the overtone number to represent Δf/n.\n",
    "    - adj_timeline\n",
    "        timeline given in the text file for the respective sensor including the origianlly recorded time in s,\n",
    "        the adjusted time as determined by a min or max within the 4min time window of the original time\n",
    "        indicating the exchange of liquids in open modules and the information given on the solution change \n",
    "      \n",
    "Comments on Coding:\n",
    "- For all variables lower_case_with_underscores are used. \n",
    "- CapWords are used for class names \n",
    "- UPPER_CASE_WITH_UNDERSCORES are used for constants.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f31d70f-a076-4528-a6d4-40f5416a3f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7086ed78deea4e0a9151ebe37bc10ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox', description='Path to QCMD open mo…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Enter the path to the folder in which the QCMD flow rawdata is saved\n",
    "'''\n",
    "\n",
    "input_widget_width = '1000px'\n",
    "description_width = '300px'\n",
    "\n",
    "path_widget = widgets.Text(\n",
    "    value=\"/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox\",\n",
    "    description=\"Path to QCMD open module rawdata:\"\n",
    ")\n",
    "\n",
    "path = None\n",
    "\n",
    "path_widget.layout.width = input_widget_width\n",
    "path_widget.style.description_width = description_width\n",
    "\n",
    "def save_path(sender):\n",
    "    global path \n",
    "    path = path_widget.value\n",
    "\n",
    "save_button = widgets.Button(description=\"Save\")\n",
    "save_button.on_click(save_path)\n",
    "\n",
    "widgets.VBox([path_widget, save_button])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a8570d-9572-40b5-b7be-049ed4cafded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "- Reads in the rawdata txt File with time in seconds as the index.\n",
    "\n",
    "Output:\n",
    "- persistentID: represents the uniqueID of the dataset - extracted from the name of the rawdata title \n",
    "- sensors: number of sensors for which data is available\n",
    "- adj_qcmd_data as long format pandas dataframe\n",
    "    - frequency and Dissipation values are normalized to start at zero by subtracting the averaged value of the first 100 data points.\n",
    "    - frequency values are divided by the overtone number to represent Δf/n.\n",
    "\n",
    "Note on Coding:\n",
    "- The rawdata is expected to be exported with column titles in the format\"Time_S\" and \"Mn_S [unit]\"\n",
    "    where M represents f for frequency or D for dissipation; n is the overtone number and S the sensor No.\n",
    "\"\"\"\n",
    "\n",
    "def process_rawdata(file_path):\n",
    "    qcmd_data_wide = pd.read_csv(file_path, sep=\"\\t\", decimal=\",\", encoding='latin-1')\n",
    "    rawdata = qcmd_data_wide\n",
    "    rawdata_array = rawdata.to_numpy()  # to save unmodified rawdata in hdf5 file later\n",
    "    rawdata_column_names = rawdata.columns\n",
    "\n",
    "    persistentID = os.path.basename(file_path)\n",
    "    persistentID = persistentID.replace(\"Data_\", \"\").replace(\"_rawdata.txt\", \"\")\n",
    "\n",
    "    sensors = {int(item.split(\"_\")[1].split(\" \")[0]) for item in qcmd_data_wide if item[0] == \"f\"}\n",
    "    overtones = {int(item.split(\"_\")[0][1:]) for item in qcmd_data_wide if item[0] == \"f\"}\n",
    "\n",
    "    print(\"Starting to process data with the persistent ID:\", persistentID)\n",
    "    print(\"In this dataset, the following overtones were measured:\")\n",
    "    print(overtones)\n",
    "    print(\"on the sensor positions\")\n",
    "    print(sensors)\n",
    "\n",
    "    norm_qcmd_data_wide = qcmd_data_wide\n",
    "\n",
    "    # the data contains a Time column for each of the sensors. \n",
    "    # Thus we must ensure that none of the columns labeled \"Time_n [s]\" - with n being the overtone number - is \"normalized\n",
    "    for col in qcmd_data_wide.columns: \n",
    "        if not re.match(r'Time_\\d+\\s+\\[s\\]', col): \n",
    "            norm_qcmd_data_wide[col] = qcmd_data_wide[col]-qcmd_data_wide[col].iloc[0:100].mean()\n",
    "            \n",
    "    # Next step: Create adj_qcmd_data in long format\n",
    "    adj_qcmd_data = pd.DataFrame(columns=[\"Time_s\", \"Overtone\", \"Sensor\", \"Deltaf_div_n_Hz\", \"Dissipation_ppm\"])\n",
    "\n",
    "    for sensor in sensors:\n",
    "        for overtone in overtones:\n",
    "            columns_selected = [\n",
    "                \"Time_\" + str(sensor) + \" [s]\",\n",
    "                \"f\" + str(overtone) + \"_\" + str(sensor) + \" [Hz]\",\n",
    "                \"D\" + str(overtone) + \"_\" + str(sensor) + \" [ppm]\"\n",
    "            ]\n",
    "\n",
    "            data_temp = qcmd_data_wide.loc[:, columns_selected]\n",
    "            data_temp.columns = [\"Time_s\", \"Deltaf_div_n_Hz\", \"Dissipation_ppm\"]\n",
    "            data_temp[\"Overtone\"] = float(overtone)\n",
    "            data_temp[\"Sensor\"] = float(sensor)\n",
    "\n",
    "            adj_qcmd_data = pd.concat([adj_qcmd_data, data_temp], ignore_index=True)\n",
    "\n",
    "    adj_qcmd_data[\"Deltaf_div_n_Hz\"] = adj_qcmd_data[\"Deltaf_div_n_Hz\"] / adj_qcmd_data[\"Overtone\"]\n",
    "    adj_qcmd_data[\"Deltaf_div_n_Hz\"] = adj_qcmd_data[\"Deltaf_div_n_Hz\"].astype(float)\n",
    "    adj_qcmd_data[\"Time_s\"] = adj_qcmd_data[\"Time_s\"].astype(float)\n",
    "    adj_qcmd_data[\"Dissipation_ppm\"] = adj_qcmd_data[\"Dissipation_ppm\"].astype(float)\n",
    "\n",
    "    adj_qcmd_data = adj_qcmd_data.dropna()\n",
    "\n",
    "    #print(\"Long-format data:\")\n",
    "    #display(adj_qcmd_data)\n",
    "    print(\"Transformed to normalized long format\")\n",
    "    \n",
    "    return persistentID, sensors, adj_qcmd_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39104f4f-bd3b-4608-95a0-a80d770f8b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "- path\n",
    "- persistenID\n",
    "- Reads in the timeline txt based on our naming conventions \n",
    "    expects title of timeline to be path+\"Data_\"+persistentID+\"_timeline.txt\"\n",
    "- adj_qcmd_data\n",
    "\n",
    "Output:\n",
    "- timeline_data_pd: adjusted time points at which liquid has been exchanged in a pandas data frame \n",
    "    column names: ['Time', 'Sensor', 'Information']\n",
    "- extreme_values_times\n",
    "  - Based on maximum and minimum values in a 4-minute timeframe of the original timepoints given in the timeline.\n",
    "  - Corresponds to max_value for SUV-containing solutions.\n",
    "  - Otherwise, calculates the difference between the extrema and the mean value over the timeframe, \n",
    "    returning the value with the larger difference.\n",
    "- corr_timeline\n",
    "  - timeline with corrected values based on extreme_values_times\n",
    "\n",
    "Note on Coding:\n",
    "- Remember to change the timeframe also in the beginning of the program if you adjust it here.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Function to find highest and lowest values within a 4-minute timeframe\n",
    "def find_extreme_values(data, timepoints, window_size=240):\n",
    "    extreme_values = []\n",
    "\n",
    "    for row in timepoints:\n",
    "        time, description = row['Time'], row['Information']\n",
    "        #print(\"Time:\", time, \"Description:\", description)\n",
    "\n",
    "        start_time = float(time) - (window_size / 2)\n",
    "        end_time = float(time) + (window_size / 2)\n",
    "\n",
    "        # Filter the data within the specified timeframe\n",
    "        data_within_timeframe = data[(data['Time_s'] >= start_time) & (data['Time_s'] <= end_time)]\n",
    "\n",
    "        if not data_within_timeframe.empty:\n",
    "            # Find the highest and lowest values within the filtered data\n",
    "            max_value_index = data_within_timeframe['Deltaf_div_n_Hz'].idxmax()\n",
    "            min_value_index = data_within_timeframe['Deltaf_div_n_Hz'].idxmin()\n",
    "\n",
    "            # Convert indices to corresponding time values\n",
    "            max_value_time = data_within_timeframe['Time_s'].loc[max_value_index]\n",
    "            min_value_time = data_within_timeframe['Time_s'].loc[min_value_index]\n",
    "\n",
    "            # Calculate mean and standard deviation\n",
    "            mean = np.mean(data_within_timeframe['Deltaf_div_n_Hz'])\n",
    "\n",
    "            # Calculate Z-scores for maximum and minimum values\n",
    "            max_value = data_within_timeframe['Deltaf_div_n_Hz'].loc[max_value_index]\n",
    "            min_value = data_within_timeframe['Deltaf_div_n_Hz'].loc[min_value_index]\n",
    "            Diff_max = max_value -  mean\n",
    "            Diff_min = mean - min_value\n",
    "\n",
    "            if Diff_max > Diff_min:\n",
    "                extreme_values.append(max_value_time)\n",
    "                #print(\"Max Value:\", max_value_time)\n",
    "            else:\n",
    "                extreme_values.append(min_value_time)\n",
    "                #print(\"Min Value:\", min_value_time)\n",
    "\n",
    "    return extreme_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_timeline_data(path, persistentID, adj_qcmd_data):\n",
    "    file_timeline = os.path.join(path, f\"Data_{persistentID}_timeline.txt\")\n",
    "\n",
    "    try:\n",
    "        with open(file_timeline, 'r', encoding='utf-8') as file:\n",
    "            timeline = file.read()\n",
    "            print(\"Timeline file read:\")\n",
    "            print(f\"Data_{persistentID}_timeline.txt\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_timeline}' was not found. Check if it was saved with the correct name\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize lists to store timeline data\n",
    "    timelines_times = []\n",
    "    timelines_sensors = []\n",
    "    timelines_speeds = []\n",
    "    timelines_infos = []\n",
    "\n",
    "    timelines = timeline.split(\"\\n\")\n",
    "    for line in timelines:\n",
    "        line = line.strip()  # Remove leading and trailing whitespace\n",
    "\n",
    "        # Check if the line is not empty\n",
    "        if line:\n",
    "            parts = line.split(\" \", 2)\n",
    "            time_str = parts[0]\n",
    "            timelines_info = parts[2]\n",
    "\n",
    "            timelines_sensor = int(parts[1].replace(\"S\", \"\"))  # Remove 'S' prefix\n",
    "\n",
    "            # Split the time string into hours, minutes, and seconds\n",
    "            time_parts = time_str.split(\":\")\n",
    "\n",
    "            # If there's no hours part, assume it's zero\n",
    "            if len(time_parts) == 2:\n",
    "                hours, minutes, seconds = 0, int(time_parts[0]), int(time_parts[1])\n",
    "            elif len(time_parts) == 3:\n",
    "                hours, minutes, seconds = map(int, time_parts)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid time format: {time_str}\")\n",
    "\n",
    "            # Convert the time to seconds\n",
    "            timelines_time = hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "            timelines_times.append(timelines_time)\n",
    "            timelines_sensors.append(timelines_sensor)\n",
    "            timelines_infos.append(timelines_info)\n",
    "\n",
    "    dtype = [('Time', int), ('adj_Time', int), ('Sensor', int), ('Information', 'S100')]\n",
    "\n",
    "    timeline_data_np = np.array(list(zip(timelines_times, timelines_times, timelines_sensors, timelines_infos)), dtype=dtype)\n",
    "            \n",
    "    extreme_values_times = find_extreme_values(adj_qcmd_data, timeline_data_np)\n",
    "    \n",
    "    timeline_data_np[\"adj_Time\"] = pd.Series(extreme_values_times)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    dtype = ['Time', 'adj_Time', 'Sensor', 'Information']\n",
    "    adj_timeline_data_pd = pd.DataFrame(timeline_data_np, columns=dtype)\n",
    "\n",
    "    adj_timeline_data_pd['Information'] = adj_timeline_data_pd['Information'].apply(lambda x: x.decode('utf-8'))\n",
    "\n",
    "    return adj_timeline_data_pd\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2289ab07-d961-424b-aa7a-3d92301de3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Data_CBP_LEAF_5303643_20250514_QCMD_rawdata.txt\n",
      "Starting to process data with the persistent ID: CBP_LEAF_5303643_20250514_QCMD\n",
      "In this dataset, the following overtones were measured:\n",
      "{3, 5, 7, 9, 11, 13}\n",
      "on the sensor positions\n",
      "{1, 2, 3, 4}\n",
      "Transformed to normalized long format\n",
      "Timeline file read:\n",
      "Data_CBP_LEAF_5303643_20250514_QCMD_timeline.txt\n",
      "Data saved as:\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514QCMD_adj_data.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S1_QCMD_adj_qcmd_data.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S1_QCMD_adj_timeline.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S2_QCMD_adj_qcmd_data.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S2_QCMD_adj_timeline.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S3_QCMD_adj_qcmd_data.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S3_QCMD_adj_timeline.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S4_QCMD_adj_qcmd_data.csv\n",
      "/Users/hboehm/Seafile/LEAF/QCMD/QCMD_data/Inbox/Data_CBP_LEAF_5303643_20250514_S4_QCMD_adj_timeline.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def identify_similar_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    similar_files = [file for file in files if \"rawdata.txt\" in file.lower()]\n",
    "    return similar_files\n",
    "\n",
    "def main():\n",
    "    rawdata_files = identify_similar_files(path)\n",
    "\n",
    "    if not rawdata_files:\n",
    "        print(f\"There are no files containing 'rawdata.txt' in the folder {path}\")\n",
    "        return\n",
    "    \n",
    "    for i, file in enumerate(rawdata_files, start=1):\n",
    "        print(f\"{i}: {file}\")\n",
    "        file_path = os.path.join(path, file)\n",
    "\n",
    "        persistentID, sensors, adj_qcmd_data = process_rawdata(file_path)\n",
    "        \n",
    "        if not persistentID:\n",
    "            print(\"Error processing rawdata. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Read timeline data using persistentID and path\n",
    "        timeline_data = read_timeline_data(path, persistentID, adj_qcmd_data)\n",
    "\n",
    "        if timeline_data is None:\n",
    "            print(\"Error reading timeline data. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Save sensor-specific adj_qcmd_data and adjusted timeline\n",
    "        print(\"Data saved as:\")\n",
    "        \n",
    "        baseID = persistentID.replace(\"_QCMD\", \"\")\n",
    "        csv_filename = os.path.join(path, \"Data_\" + baseID)\n",
    "        adj_qcmd_data.to_csv(csv_filename + \"_QCMD_adj_data.csv\", index=True)\n",
    "        print(csv_filename + \"QCMD_adj_data.csv\")\n",
    "\n",
    "        # Loop through the sensors\n",
    "        for sensor in sensors:\n",
    "            # Save sensor-specific adjusted QCM-D data as a CSV file\n",
    "            sensor_qcmd_data = adj_qcmd_data[adj_qcmd_data[\"Sensor\"] == sensor].copy()\n",
    "            del sensor_qcmd_data[\"Sensor\"]\n",
    "            adj_qcmd_data_name = f\"{csv_filename}_S{sensor}_QCMD_adj_qcmd_data.csv\"\n",
    "            sensor_qcmd_data.to_csv(adj_qcmd_data_name, index=True)\n",
    "            print(adj_qcmd_data_name)\n",
    "\n",
    "            # Save sensor-specific adjusted timeline as a CSV file\n",
    "            adj_timeline_filename = f\"{csv_filename}_S{sensor}_QCMD_adj_timeline.csv\"\n",
    "            adj_sensor_timeline = timeline_data[timeline_data[\"Sensor\"] == sensor].copy()\n",
    "            del adj_sensor_timeline[\"Sensor\"]\n",
    "            adj_sensor_timeline.to_csv(adj_timeline_filename, index=True)\n",
    "            print(adj_timeline_filename)\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da54990-b07d-4f5a-a545-4f6546e6fedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
